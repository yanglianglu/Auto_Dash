# -*- coding: utf-8 -*-
"""CS410_TopicModelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r-AHkhACx2x6trti-Fw9evBoGSwiCtUL
"""
import matplotlib
import nltk
import pickle
import numpy as np
import re
import pandas as pd
import dash_bootstrap_components as dbc

from gensim import corpora
from collections import Counter
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from gensim.test.utils import common_texts
from gensim.corpora.dictionary import Dictionary
import gensim.models
from sklearn.datasets import fetch_20newsgroups
from nltk.corpus import stopwords
matplotlib.use('Agg')  # Use a non-interactive backend

from wordcloud import WordCloud, STOPWORDS
import matplotlib.colors as mcolors
from matplotlib import pyplot as plt
import src.preprocessing.text_preprocessing as tp

nltk.download('wordnet')
nltk.download('stopwords')


# Load text data
# docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']


# Task 1: Initiate a new topic model
def fit_topic_model(docs):
    # Lemmatize document from your text data
    lemmatizer = tp.Lemmatizer()
    docss = lemmatizer.lemmatize_document(docs)

    # Create a corpus from a list of texts
    common_dictionary = Dictionary(docss)
    common_corpus = [common_dictionary.doc2bow(text) for text in docss]

    # Train the model on the corpus.
    # corpus has to be provided as a keyword argument, as they are passed through to the children.
    elda = gensim.models.LdaModel(corpus=common_corpus,
                                  id2word=common_dictionary,
                                  num_topics=4,
                                  random_state=100,
                                  # update_every=1,
                                  # chunksize=100,
                                  passes=10,
                                  # alpha='auto',
                                  per_word_topics=False)
    return elda, common_corpus, docss


# Task 2: Topics and its list of documents
# One dominant topic per document
# input: [common_corpus, docss, fitted_topic_model]
# output: [topic_id, list of doc ids]
def find_topics_document_unique(corpus, texts, ldamodel=None):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = pd.concat(
                    [sent_topics_df, pd.Series([i, int(topic_num), round(prop_topic, 4), topic_keywords]).to_frame().T])
            else:
                break
    sent_topics_df.reset_index(inplace=True, drop=True)
    sent_topics_df.columns = ['Doc_ID', 'Dominant_Topic', 'Percent_Contribution', 'Topic_Keywords']
    # Add original text to the end of the output
    contents = pd.Series(texts)

    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1).groupby("Dominant_Topic")["Doc_ID"].apply(
        list).reset_index()
    # print(sent_topics_df)
    return sent_topics_df


# Task 3: Topics and its list of documents
# Multiple topics per document if percentage contribution > 0.1
# input: [common_corpus, docss, fitted_topic_model]
# output: [topic_id, list of doc ids]
def find_topics_document_multiple(corpus, texts, ldamodel=None):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if prop_topic > 0.1:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = pd.concat(
                    [sent_topics_df, pd.Series([i, int(topic_num), round(prop_topic, 4), topic_keywords]).to_frame().T])
            else:
                break
    sent_topics_df.reset_index(inplace=True, drop=True)
    sent_topics_df.columns = ['Doc_ID', 'Dominant_Topic', 'Percent_Contribution', 'Topic_Keywords']
    # Add original text to the end of the output
    contents = pd.Series(texts)

    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1).groupby("Dominant_Topic")["Doc_ID"].apply(
        list).reset_index()
    # print(sent_topics_df)
    return sent_topics_df


# a,b,c = fit_topic_model(docs)
# result = find_topics_document_unique(b, c, a)
# result


# Task 4. Plot Wordcloud of Top N words in each topic
def plot_word_cloud_word_weight_per_topic(ldamodel, figurename='word_cloud_per_topic.png'):
    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'

    cloud = WordCloud(background_color='white',
                      width=2500,
                      height=1800,
                      max_words=10,
                      colormap='tab10',
                      color_func=lambda *args, **kwargs: cols[i],
                      prefer_horizontal=1.0)

    topics = ldamodel.show_topics(4, formatted=False)

    fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)

    for i, ax in enumerate(axes.flatten()):
        fig.add_subplot(ax)
        topic_words = dict(topics[i][1])
        cloud.generate_from_frequencies(topic_words, max_font_size=300)
        plt.gca().imshow(cloud)
        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))
        plt.gca().axis('off')

    plt.subplots_adjust(wspace=0, hspace=0)
    plt.axis('off')
    plt.margins(x=0, y=0)
    plt.tight_layout()
    plt.savefig(figurename)


# plot_word_cloud_word_weight_per_topic(a)
# topics = a.show_topics(num_words=10, formatted=False)


# Task 5: Word Count of Top N words in each topic among topic related documents
def get_topic_word_weight_related_document(ldamodel, texts, doc_list_per_topic):
    topics = ldamodel.show_topics(formatted=False)
    out = []
    topic_set = set([])
    for i, topic in topics:
        data_flat = [w for j, w_list in enumerate(texts) if j in doc_list_per_topic.iloc[i, 1] for w in w_list]
        counter = Counter(data_flat)
        for word, weight in topic:
            out.append([word, i, weight, counter[word]])
            topic_set.add(i)

    df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])
    return df, topic_set


# df, topic_set = get_topic_word_weight_related_document(a, c, result)


# Task 6: Word Count of Top N words in each topic
def get_topic_word_weight(ldamodel, texts):
    topics = ldamodel.show_topics(formatted=False)
    data_flat = [w for w_list in texts for w in w_list]
    counter = Counter(data_flat)

    out = []
    topic_set = set([])
    for i, topic in topics:
        for word, weight in topic:
            out.append([word, i, weight, counter[word]])
            topic_set.add(i)

    df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])
    return df, topic_set


# Task 7: Barplot Word Count and Weights of Topic Keywords
# input: output of get_topic_word_weight, get_topic_word_weight_related_document
def plot_topic_word_wordcount_weight(df, topic_set):
    fig, axes = plt.subplots(2, 2, figsize=(10, 6), sharey=True, dpi=160)
    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]

    for i, ax in enumerate(axes.flatten()):
        ax.bar(x='word', height="word_count", data=df.loc[df.topic_id == list(topic_set)[i], :], color=cols[i],
               width=0.5, alpha=0.3, label='Word Count')
        ax_twin = ax.twinx()
        ax_twin.bar(x='word', height="importance", data=df.loc[df.topic_id == list(topic_set)[i], :], color=cols[i],
                    width=0.2, label='Weights')
        ax.set_ylabel('Word Count', color=cols[i])
        left_ylim = df['importance'].max() + 0.02
        right_ylim = df['word_count'].max() + 100
        ax_twin.set_ylim(0, left_ylim)
        ax.set_ylim(0, right_ylim)
        ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)
        ax.tick_params(axis='y', left=True)
        ax.set_xticklabels(df.loc[df.topic_id == list(topic_set)[i], 'word'], rotation=30, horizontalalignment='right')
        ax.legend(loc='upper left')
        ax_twin.legend(loc='upper right')

    fig.tight_layout(w_pad=2)
    fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)
    plt.savefig('plot_topic_word_wordcount_weight.png')


# plot_topic_word_wordcount_weight(df, topic_set)
