# -*- coding: utf-8 -*-
"""CS410_TopicModelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r-AHkhACx2x6trti-Fw9evBoGSwiCtUL
"""

import nltk
import pickle
import numpy as np
import re
import pandas as pd

from gensim import corpora
from collections import Counter
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from gensim.test.utils import common_texts
from gensim.corpora.dictionary import Dictionary
import gensim.models
from sklearn.datasets import fetch_20newsgroups
from nltk.corpus import stopwords


nltk.download('wordnet')
nltk.download('stopwords')

def remove_special_characters_and_numbers(text):
    # Remove special characters and numbers using regular expressions
    cleaned_text = re.sub(r'[^A-Za-z\s]', '', text)  # Remove special characters
    cleaned_text = re.sub(r'\b\d+\b', '', cleaned_text)  # Remove standalone numbers

    return cleaned_text

class Tokenizer:
    def __init__(self):
        self._tokenizer = nltk.RegexpTokenizer(r'\w+')

    def tokenize(self, document: str) -> list:
        return self._tokenizer.tokenize(document)

class Lemmatizer:
    def __init__(self):
        self._lemmatizer = WordNetLemmatizer()
        self._tokenizer = Tokenizer()
        self._stopwords = set(stopwords.words("english"))
        self._custom_stop_words = ['the', 'and', 'is', 'in', 'it', 'of', 'for', 'to', 'with', 'on']
        self._stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])


    def _tokenize(self, document: str) -> list:
        return self._tokenizer.tokenize(document)

    def lemmatize_word(self, word: str, pos=None) -> str:
        return self._lemmatizer.lemmatize(word, pos) if pos is not None else self._lemmatizer.lemmatize(word)

    def lemmatize_sentence(self, sentence: str, pos=None) -> str:
        result = []
        sentence = sentence.lower()
        for word in self._tokenize(sentence):
            word = remove_special_characters_and_numbers(word)
            if word.lower() in self._stopwords or word.lower() in self._custom_stop_words or len(word) < 3:
                continue
            if pos is not None:
                result.append(self.lemmatize_word(word, pos))
            else:
                result.append(self.lemmatize_word(word))
        return result

    def lemmatize_document(self, document: list) -> str:
        result = []
        for line in document:
            result.append(self.lemmatize_sentence(line))
        return result

message = "Have no fear of perfection. You'll never reach it ðŸ”¥"
tokenizer = Tokenizer()
print(tokenizer.tokenize(message))

message = "Have no fear of perfection. You'll never reach it ðŸ”¥"
lemmatizer = Lemmatizer()
print(lemmatizer.lemmatize_sentence(message))

## Load text data
docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']

## import pandas as pd
## df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')
## df = df.loc[df.target_names.isin(['soc.religion.christian', 'rec.sport.hockey', 'talk.politics.mideast', 'rec.motorcycles']) , :]

# Lemmatize document from your text data
lemmatizer = Lemmatizer()
docss = lemmatizer.lemmatize_document(docs)

# Create a dictionary from a list of texts
# Create a corpus from a list of texts
common_dictionary = Dictionary(docss)
common_corpus = [common_dictionary.doc2bow(text) for text in docss]

# Train the model on the corpus.
# corpus has to be provided as a keyword argument, as they are passed through to the children.
elda = gensim.models.LdaModel(corpus=common_corpus,
                                           id2word=common_dictionary,
                                           num_topics=20,
                                           random_state=100,
                                           #update_every=1,
                                           #chunksize=100,
                                           passes=10,
                                           #alpha='auto',
                                           per_word_topics=False)

# Get the topics and their top words
topics = elda.print_topics(num_words= 10)
for topic in topics:
    print(topic)

# Create a new folder to store model
import os
folder_path = '/content/my_topic_model'  # Replace 'my_folder' with the desired folder name
os.makedirs(folder_path, exist_ok=True)

# Save the topic model to model_path
model_path = folder_path + "/lda_model"
elda.save(model_path)

# Get the topic modelling for a specific document
document = docss[0]
topic_distribution = elda[common_dictionary.doc2bow(document)]
print(topic_distribution)

elda.get_topic_terms(0)

len(common_dictionary.token2id)

len(docss[0])

row_list = elda[common_corpus[0]]

row = sorted(row_list, key=lambda x: (x[1]), reverse=True)

row

sent_topics_df = pd.DataFrame()
for j, (topic_num, prop_topic) in enumerate(row):
           if j == 0:  # => dominant topic
               wp = elda.show_topic(topic_num)
               topic_keywords = ", ".join([word for word, prop in wp])
               print(topic_keywords)
               sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
           else:
               break

sent_topics_df

elda.get_topics()[0]

elda.show_topic(topic_num)

topic_num

elda.get_document_topics()

# the dominant topic and its percentage contribution in each document
def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)

# Creating Parameters for pyldavis
import pyLDAvis.gensim

# Initializing pyldavis
lda_panel = pyLDAvis.gensim.prepare(topic_model=elda, corpus=common_corpus, dictionary=common_dictionary)

# Displaying pyldavis
pyLDAvis.enable_notebook()
pyLDAvis.display(lda_panel)

